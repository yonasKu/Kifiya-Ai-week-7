{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('../scripts'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from telethon import TelegramClient, types  # Import types for media handling\n",
    "import json\n",
    "\n",
    "# Step 1: Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Step 2: Fetch API ID and API Hash from environment variables\n",
    "api_id = os.getenv('TELEGRAM_API_ID')\n",
    "api_hash = os.getenv('TELEGRAM_API_HASH')\n",
    "\n",
    "# Step 3: Initialize the Telegram client (with extended timeout)\n",
    "client = TelegramClient('scraper_session_alt', api_id, api_hash, timeout=60)\n",
    "\n",
    "# Step 4: Define a function to fetch messages from a channel\n",
    "async def fetch_messages(channel_link, limit=100):\n",
    "    messages_data = []\n",
    "\n",
    "    async for message in client.iter_messages(channel_link, limit=limit):\n",
    "        message_dict = {\n",
    "            \"sender\": message.sender_id,\n",
    "            \"timestamp\": message.date.isoformat(),\n",
    "            \"content\": message.message,\n",
    "            \"media\": None  # Initialize with None, updated later if media exists\n",
    "        }\n",
    "        \n",
    "        # Check if the message contains media\n",
    "        if message.media:\n",
    "            # Handle different media types\n",
    "            if isinstance(message.media, types.MessageMediaPhoto):\n",
    "                media_file = await client.download_media(message.media, file=f\"./media/photo_{message.id}\")\n",
    "                message_dict[\"media\"] = media_file  # Save the path to the downloaded photo\n",
    "            elif isinstance(message.media, types.MessageMediaDocument):\n",
    "                media_file = await client.download_media(message.media, file=f\"./media/doc_{message.id}\")\n",
    "                message_dict[\"media\"] = media_file  # Save the path to the downloaded document\n",
    "            elif isinstance(message.media, types.MessageMediaWebPage):\n",
    "                # Handle web pages or other media types differently if needed\n",
    "                message_dict[\"media\"] = f\"Web page URL: {message.media.webpage.url}\" if message.media.webpage else \"Unknown webpage\"\n",
    "\n",
    "        messages_data.append(message_dict)\n",
    "\n",
    "    return messages_data\n",
    "\n",
    "# Step 5: Define a function to fetch data from multiple channels\n",
    "async def fetch_all_channels(channels, limit=100):\n",
    "    for channel in channels:\n",
    "        print(f\"Fetching data from {channel}...\")\n",
    "        data = await fetch_messages(channel, limit)\n",
    "        file_name = f'{channel.split(\"/\")[-1]}_messages.json'\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Saved {len(data)} messages from {channel} to {file_name}\")\n",
    "\n",
    "# Step 6: List the Telegram channels you want to scrape\n",
    "channels = [\n",
    "    'https://t.me/forfreemarket'\n",
    "]\n",
    "\n",
    "# Step 7: Use await directly in Jupyter notebook\n",
    "async def main():\n",
    "    async with client:\n",
    "        await fetch_all_channels(channels, limit=4000)\n",
    "\n",
    "# Instead of client.loop.run_until_complete(main()), just use await main()\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sender                  timestamp  \\\n",
      "0 -1001866685679  2024-08-06T10:13:04+00:00   \n",
      "1 -1001866685679  2024-08-06T10:12:52+00:00   \n",
      "2 -1001866685679  2024-08-06T10:12:39+00:00   \n",
      "3 -1001866685679  2024-08-06T10:12:25+00:00   \n",
      "4 -1001866685679  2024-08-06T10:12:19+00:00   \n",
      "\n",
      "                                             content                  media  \n",
      "0  [tacketa, size, 3940414243, 1600, free, delive...  ./media\\photo_768.jpg  \n",
      "1  [tacketa, size, 3940414243, 1600, free, delive...  ./media\\photo_766.jpg  \n",
      "2  [tacketa, size, 3940414243, 1600, free, delive...  ./media\\photo_764.jpg  \n",
      "3  [tacketa, size, 3940414243, 1600, free, delive...  ./media\\photo_762.jpg  \n",
      "4  [tacketa, size, 3940414243, 1600, free, delive...  ./media\\photo_760.jpg  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Function to check if a word contains Amharic characters\n",
    "def contains_amharic(text):\n",
    "    return bool(re.search(r'[\\u1200-\\u137F]', text))\n",
    "\n",
    "# Function to preprocess the text data, keeping only Amharic words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)  # Remove mentions and hashtags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    \n",
    "    # Tokenize the text (split into words)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Keep only tokens that contain Amharic characters\n",
    "    amharic_tokens = [word for word in tokens if contains_amharic(word)]\n",
    "    \n",
    "    return amharic_tokens\n",
    "\n",
    "# Load the JSON file containing the messages\n",
    "def load_data(file_name):\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Process the loaded data, keeping only messages with Amharic content\n",
    "def process_data(data):\n",
    "    processed_data = []\n",
    "    for message in data:\n",
    "        if 'content' in message and message['content'] and contains_amharic(message['content']):\n",
    "            # Preprocess the content, keeping only Amharic words\n",
    "            amharic_content = preprocess_text(message['content'])\n",
    "            if amharic_content:  # If there are still Amharic words after filtering\n",
    "                processed_data.append({\n",
    "                    'sender': message['sender'],\n",
    "                    'timestamp': message['timestamp'],\n",
    "                    'content': ' '.join(amharic_content),  # Join tokens back into a string\n",
    "                    'media': message.get('media', None)  # Handle case where 'media' might not exist\n",
    "                })\n",
    "    return processed_data\n",
    "\n",
    "# Example usage\n",
    "file_name = 'chuchushoes_messages.json'  # Replace with your actual file name\n",
    "raw_data = load_data(file_name)\n",
    "processed_data = process_data(raw_data)\n",
    "\n",
    "# Convert to DataFrame for easy analysis and manipulation\n",
    "df = pd.DataFrame(processed_data)\n",
    "print(df.head())  # Display the first few rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data to a CSV file\n",
    "df.to_csv('processed_chuchushoes_messages.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Normalization: Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters, links, and emojis\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)  # Remove mentions and hashtags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    \n",
    "    # Tokenization: Split the text into words (you can also use nltk for more advanced tokenization)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Further processing for Amharic (e.g., stemming or specific handling can be added here)\n",
    "    # For example: handle special Amharic characters, etc.\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries if needed\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from telethon import TelegramClient, types\n",
    "import nltk\n",
    "\n",
    "# Download NLTK punkt tokenizer if not already done\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch API ID and API Hash from environment variables\n",
    "api_id = os.getenv('TELEGRAM_API_ID')\n",
    "api_hash = os.getenv('TELEGRAM_API_HASH')\n",
    "\n",
    "# Initialize the Telegram client (with extended timeout)\n",
    "client = TelegramClient('scraper_session_alt', api_id, api_hash, timeout=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the punkt resource if not already available\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_amharic(text):\n",
    "    # Tokenize text using NLTK's word_tokenize function\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return token\n",
    "\n",
    "# Tokenization function for Amharic (or any text)\n",
    "def tokenize_amharic(text):\n",
    "    # Basic tokenization using whitespace and punctuation\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Normalization function for Amharic text\n",
    "def normalize_amharic(text):\n",
    "    # Normalize to NFC form (for consistent Unicode encoding)\n",
    "    normalized_text = unicodedata.normalize('NFC', text)\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import unicodedata\n",
    "\n",
    "# Download the punkt resource if not already available\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Clean and structure data by separating metadata and content\n",
    "def clean_and_structure(data):\n",
    "    structured_data = []\n",
    "\n",
    "    for message in data:\n",
    "        # Clean message content by removing URLs, special characters, and extra whitespaces\n",
    "        cleaned_content = re.sub(r'http\\S+', '', message['content'])  # Remove URLs\n",
    "        cleaned_content = re.sub(r'\\W+', ' ', cleaned_content)  # Remove special characters\n",
    "        cleaned_content = cleaned_content.strip()\n",
    "\n",
    "        # Tokenize and normalize\n",
    "        tokens = tokenize_amharic(cleaned_content)\n",
    "        normalized_content = normalize_amharic(' '.join(tokens))\n",
    "        \n",
    "        # Structure the data into a dictionary\n",
    "        structured_message = {\n",
    "            \"sender\": message[\"sender\"],\n",
    "            \"timestamp\": message[\"timestamp\"],\n",
    "            \"tokens\": tokens,\n",
    "            \"normalized_content\": normalized_content,\n",
    "            \"media\": message[\"media\"]\n",
    "        }\n",
    "        structured_data.append(structured_message)\n",
    "    \n",
    "    return structured_data\n",
    "\n",
    "# Tokenization function for Amharic (or any text)\n",
    "def tokenize_amharic(text):\n",
    "    # Tokenize text using NLTK's word_tokenize function\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens  # Fixed return value (previously was returning 'token' by mistake)\n",
    "\n",
    "# Normalization function for Amharic text\n",
    "def normalize_amharic(text):\n",
    "    # Normalize to NFC form (for consistent Unicode encoding)\n",
    "    normalized_text = unicodedata.normalize('NFC', text)\n",
    "    return normalized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save structured data to a CSV file\n",
    "def save_to_csv(data, output_file):\n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8-sig')  # Use utf-8-sig to handle Amharic characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch messages from a Telegram channel\n",
    "async def fetch_messages(channel_link, limit=100):\n",
    "    messages_data = []\n",
    "\n",
    "    async for message in client.iter_messages(channel_link, limit=limit):\n",
    "        message_dict = {\n",
    "            \"sender\": message.sender_id,\n",
    "            \"timestamp\": message.date.isoformat(),\n",
    "            \"content\": message.message,\n",
    "            \"media\": None  # Initialize with None, updated later if media exists\n",
    "        }\n",
    "        \n",
    "        # Check if the message contains media\n",
    "        if message.media:\n",
    "            # Handle different media types\n",
    "            if isinstance(message.media, types.MessageMediaPhoto):\n",
    "                media_file = await client.download_media(message.media, file=f\"./media/photo_{message.id}\")\n",
    "                message_dict[\"media\"] = media_file  # Save the path to the downloaded photo\n",
    "            elif isinstance(message.media, types.MessageMediaDocument):\n",
    "                media_file = await client.download_media(message.media, file=f\"./media/doc_{message.id}\")\n",
    "                message_dict[\"media\"] = media_file  # Save the path to the downloaded document\n",
    "            elif isinstance(message.media, types.MessageMediaWebPage):\n",
    "                # Handle web pages or other media types differently if needed\n",
    "                message_dict[\"media\"] = f\"Web page URL: {message.media.webpage.url}\" if message.media.webpage else \"Unknown webpage\"\n",
    "\n",
    "        messages_data.append(message_dict)\n",
    "\n",
    "    return messages_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch messages from multiple channels, preprocess them, and save to CSV\n",
    "async def fetch_all_channels(channels, limit=100):\n",
    "    for channel in channels:\n",
    "        print(f\"Fetching data from {channel}...\")\n",
    "        raw_data = await fetch_messages(channel, limit)\n",
    "        structured_data = clean_and_structure(raw_data)\n",
    "        \n",
    "        # Save the processed data to CSV\n",
    "        output_file = f'{channel.split(\"/\")[-1]}_messages.csv'\n",
    "        save_to_csv(structured_data, output_file)\n",
    "        \n",
    "        print(f\"Saved {len(structured_data)} messages from {channel} to {output_file}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the Telegram channels you want to scrape\n",
    "channels = [\n",
    "    'https://t.me/chuchushoes'\n",
    "]\n",
    "\n",
    "# Main function to run the Telegram scraper and save output as CSV\n",
    "async def main():\n",
    "    async with client:\n",
    "        await fetch_all_channels(channels, limit=1000)\n",
    "\n",
    "# Run the main function\n",
    "await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
